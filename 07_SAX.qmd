# Suposición de otro tipo de codificación de eventos  

Se pretende en este apartado utilizar otro tipo de codificación de eventos 
para el cálculo de las probabilidades y la información mutua. 

En nuestro caso vamos a proponer el sistema de codificación SAX (ver artículo @SAX). 

## Descripción del sistema de codificación SAX  

SAX permite reducir una serie temporal de tamaño $n$
a otra de tamaño $w$, usualmente $w << n$. El tamaño del alfabeto, $a$, será  con la restricción de que 
$a > 2$. 

Los pasos que se van a seguir son: 

1. Transformación de los datos en **PAA** *Piecewise Aggregate Approximation*

Llevaremos a cabo dicha transformación de la siguiente manera: 
- En primer lugar, representaremos una serie temporal $C$ de longitud $n$, en un vector $\bar C$ de dimensión $w$. Para ello, 
tomaremos que el elemento i-ésimo de $\bar C$ viene definido como:

$$
\bar c_i
= 
\frac{w}{n}
\sum_{j = 
\frac{n}{w}(i-1)+1
}^{\frac{n}{w}i}
c_j.
$$

Cabe destacar, que esto no es más que una media de los elementos adyacentes. 


2. **Proceso de discretización** 

Cada componente de la nueva señal transformada será mapeada por 
un símbolo en función del rango de valor en que se encuentre. 

Para ello definiremos los *breakpoints*, que no son más que una lista de números $B= b_1, \ldots, b_{a-1}$ de tal forma que su area bajo una normal $\mathcal{N}(0,1)$ será para cada uno de ellos 
$\frac{1}{a}$.
Además se tiene que $b_0 = -\infty$ y  $b_a = +\infty$. 

De esta manera formaremos las palabras $\hat C$
que vendrán dadas del siguiente modo: 
$$
\hat c_i = \alpha_j
\text{, sii  }
\beta_{j-1} \leq \beta_j
$$

obteniendo, así, la nueva señal. 



## Observaciones

Notemos que se tienen dos variables libres en este sistema: 

-  $w$ como el tamaño de PAA, que de manera implícita debe de ser un divisor de $n$, el tamaño original, para mayor comodidad. 

- $a$ como el tamaño del alfabeto. 

## Sobre nuestra implementación 

Se puede encontrar la implementación de los algoritmos descritos en 
`src/SAX.py`, en ellos se encuentran fielmente escritos los pasos mencionados.  

Cabe destacar que  se han tomado las siguientes decisiones en diseño: 

- Por eficiencia y para poder reutilizar el código de información mutua se ha tomado por alfabeto un subconjunto de tamaño $a$ de los números reales. 



## Experimentación con SAX  

### Descripción del experimento   

Se pretende calcular la información mutua en distintas circunstancias y compararlas
con los datos anteriores. 

Con este fin, $w$, que será el tamaño de la nueva señal de un trozo $T$, vendrá dado por 
$$
w = \frac{\text{Tamaño de la señal del trozo } T}{ w'}
$$
donde $w'$ es el tamaño de palabra que se usó en el trozo $T$ de los apartados anteriores. 


Por limitaciones computacionales se han seleccionado las siguientes combinaciones de prueba: 


| Trozo | Tamaños $w'$ | 
| :-: | :-: | 
| C | 11,87,187 | 
|R| 17,42,125|  
|G|66, 93,248| 

: Combinaciones de tamaño de palabra $w'$ para SAX que se van a realizar {#tbl-sax-palabra} 

-Los tamaños de alfabeto: $1,2,5,10,20$ y $50$
independientemente del trozo que sea. 

-Los valores de bit con los que calcular la información mutua: $1,2,3,4,5,6,7$ y $8$, independientemente del trozo que sea.

Para el experimento que vamos a realizar, se han llevado a cabo todas las combinaciones posibles
 de estos parámetros, calculando, asimismo, la información mutua pertinente. 

Puede ejecutarse el experimento realizando el comando `make calcular_sax` o directamente escribiendo `python src/experiment_sax_mi.py`. 

### Resultados obtenidos   

A continuación procedemos a mostrar y comentar los resultados que se han obtenido para este tipo de codificación:
### Trozo C   

Pueden encontrarse los resultados completos en el apéndice, sección
\ref{appendix:codification_sax_a}. 

A continuación, se muestra una representación gráfica de los mismos con un mapa de color, tal y como se puede observar en la figura @fig-A-mapa-calor. En dicha gráfica interpretaremos que, a más claro mayor es la información mutua.

:::{#fig-A-mapa-calor  layout-ncol=3}

![](img/SAX/trozoC/heatmap/mapaCalor_bits=1.png)

![](img/SAX/trozoC/heatmap/mapaCalor_bits=2.png)

![](img/SAX/trozoC/heatmap/mapaCalor_bits=3.png)

![](img/SAX/trozoC/heatmap/mapaCalor_bits=4.png)

![](img/SAX/trozoC/heatmap/mapaCalor_bits=5.png)

![](img/SAX/trozoC/heatmap/mapaCalor_bits=6.png)

![](img/SAX/trozoC/heatmap/mapaCalor_bits=7.png)

![](img/SAX/trozoC/heatmap/mapaCalor_bits=8.png)

Mapa de color de la información mutua variando en función del número de 
bits, tamaño de palabra $w'$ y tamaño de alfabeto $a$.
:::


A la vista de los resultados, vemos que por lo general, aumentar el 
ancho de ventana mejora los resultados, pero que llegado a un cierto punto, dichos resultados comienzan a empeorar. 

El ancho de ventana en este problema se puede ampliar en dos aspectos y tiene diferentes matices:  

- **Tamaño de palabra para el cálculo de la información mutua**, lo que nosotros hemos denominado durante todo el experimento como bits $b$. Tiene en cuenta los eventos temporales en situaciones contiguas de tiempo. 

- **Tamaño de palabra de reducción**, esto es lo que nosotros hemos denotado como $w'$, reduce a un símbolo la media de valores de esa palabra. 

Una moraleja que se obtiene de aquí es que  **aumentar el número de bits no significa mayor cantidad de información mutua**. Fijadas la palabra y el tamaño del alfabeto, 
al  hacer crecer el número de bits se aprecia una tendencia creciente de la información mutua, salvo en ciertas ocasiones. 

Como muestra de ello se tienen las siguiente combinaciones, para las  que aumentar el número de bits no implica una mejora en la información mutua: 

| words | alphabet size |   
| :-: | :-: | 
| 87 | 20 |  
| 187 | 20 | 
| 87 | 50 | 
| 187 | 50 | 

: Tamaño de palabra y tamaño de alfabeto para los cuales la IM no aumenta con el número de bits {#tbl-resultados-sax-trozo-a-bits}

Queda reflejado su valor en la @fig-SAX-C-IM-bits

:::{#fig-SAX-C-IM-bits  layout-ncol=2}

![](img/SAX/trozoC/bits/IM_variando_bits_w=11.png)

![](img/SAX/trozoC/bits/IM_variando_bits_w=87.png)

![](img/SAX/trozoC/bits/IM_variando_bits_w=187.png)

Información mutua variando bits para para trozo C
:::

Si ordenamos los parámetros por su eficiencia mutua puede encontrar a los diez primeros en la @tbl-resultados-sax-trozo-a-bits-top-im .  
Aquí vemos que el máximo de IM se alcanza para 8 bits, con un tamaño de palabra 11 y tamaño de alfabeto 50. 



bits|words|alphabet_size|IM(LP,VD)  
--- | --- | --- | ---   
8 | 11 | 50 | 17.477040  
7 | 11 | 50 | 17.464702  
6 | 11 | 50 | 17.349276  
5 | 11 | 50 | 16.866878  
8 | 11 | 20 | 16.268008  
7 | 11 | 20 | 15.786441  
5 | 87 | 50 | 15.349300  
6 | 87 | 50 | 15.183218  
4 | 87 | 50 | 15.093791  
6 | 87 | 20 | 15.058446  

: Combinaciones de información mutua máxima  {#tbl-resultados-sax-trozo-a-bits-top-im}

A la vista de estos resultados podemos observar que para tamaños de bit lo suficientemente grandes, los parámetros clave han sido el tamaño de palabra $11$ y un alfabeto de tamaño $50$. 


La tendencia a mejorar con el tamaño del alfabeto puede observarse gráficamente en la @fig-SAX-C-IM-alfabeto : 

:::{#fig-SAX-C-IM-alfabeto  layout-ncol=3}

![](img/SAX/trozoC/alfabeto/IM_variando_alfabeto_w=187.png)
 
![](img/SAX/trozoC/alfabeto/IM_variando_alfabeto_w=87.png)
 
![](img/SAX/trozoC/alfabeto/IM_variando_alfabeto_w=11.png)
 
:::

A continuación, en la @fig-SAX-C-IM-palabra  se muestra cómo influye el tamaño de palabra 

:::{#fig-SAX-C-IM-palabra  layout-ncol=3}

![](img/SAX/trozoC/palabra/IM_variando_palabra_a=1.png) 

![](img/SAX/trozoC/palabra/IM_variando_palabra_a=2.png)

![](img/SAX/trozoC/palabra/IM_variando_palabra_a=5.png)

![](img/SAX/trozoC/palabra/IM_variando_palabra_a=10.png)

![](img/SAX/trozoC/palabra/IM_variando_palabra_a=20.png)

![](img/SAX/trozoC/palabra/IM_variando_palabra_a=50.png)

Información mutua variando bits para para trozo C
:::


Las conclusiones que se sacan es que cuando muchos 
*samples* se ven involucrados *la información se diluye* y por tanto la información mutua baja. 
Como era de esperar, al añadir más símbolos, la información mutua mejora en comparación con la primera codificación, la idea intuitiva de esto subyace en que tenemos *más pistas* de lo que va a ocurrir. 

Por otro lado, un problema que tiene es que no contempla aportar la información para los spikes.

## Comparativa  

En este apartado realizaremos una comparación de los resultados obtenidos con SAX
y con la codificación de los anteriores apartados.
### Ventajas  

- La **reducción de tamaño** de la señal  permite un tratamiento más rápido. 
- La discretización de los valores que puede tomar la señal, lo cuál simplifica en gran medida su tratamiento. 

### Desventajas  

- La pérdida de información, supone un gran problema para nuestro estudio.
- La rigidez a la hora de calcular $\bar C$ ya que solo es el promedio de los datos de una palabra.
- La selección de hiperparámetros libres, se desconoce un método eficaz y riguroso como criterio de selección o descarte de los hiperparámetros $a$ y
$w$. 

## Trozo  R 

Se ha repetido el mismo análisis para el resto de trozos R y G, siendo las conclusiones equivalentes para estos dos últimos.  

Se pueden encontrar los resultados numéricos del trozo R en el apéndice \ref{appendix:codification_sax_r} (o en la carpeta `experiment_results/SAX`).

Además, se puede contemplar el mapa de calor de la información mutua en la @fig-R-mapa-calor  

:::{#fig-R-mapa-calor  layout-ncol=3}

![](img/SAX/trozoR/heatmap/mapaCalor_bits=1.png)

![](img/SAX/trozoR/heatmap/mapaCalor_bits=2.png)

![](img/SAX/trozoR/heatmap/mapaCalor_bits=3.png)

![](img/SAX/trozoR/heatmap/mapaCalor_bits=4.png)

![](img/SAX/trozoR/heatmap/mapaCalor_bits=5.png)

![](img/SAX/trozoR/heatmap/mapaCalor_bits=6.png)

![](img/SAX/trozoR/heatmap/mapaCalor_bits=7.png)

![](img/SAX/trozoR/heatmap/mapaCalor_bits=8.png)

Mapa de color de la información mutua variando en función del número de 
bits, tamaño de palabra $w'$ y tamaño de alfabeto $a$.
:::


Los diez mejores valores de información mutua que hemos obtenido han sido 

bits | words | alphabet_size | IM(LP,VD)  
--- | --- | --- | ---     
6 | 17 | 5 | 6.977234
7 | 17 | 5 | 6.923350
8 | 17 | 5 | 6.813161
5 | 17 | 5 | 6.790366
8 | 17 | 2 | 6.218410
7 | 17 | 2 | 6.061772
5 | 42 | 5 | 6.053265
6 | 42 | 5 | 5.940968
7 | 42 | 5 | 5.754503
7 | 42 | 2 | 5.612413

: Combinaciones de información mutua máxima para el trozo R  {#tbl-resultados-sax-trozo-r-bits-top-im}

## Trozo G
Para este trozo, se pueden encontrar los resultados numéricos en el apéndice \ref{appendix:codification_sax_g} (o en la carpeta `experiment_results/SAX`).

Del mismo modo, se puede contemplar el mapa de calor de la información mutua en la @fig-G-mapa-calor , preste atención a los valores del rango con cuidado. 

:::{#fig-G-mapa-calor  layout-ncol=3}

![](img/SAX/trozoG/heatmap/mapaCalor_bits=1.png)

![](img/SAX/trozoG/heatmap/mapaCalor_bits=2.png)

![](img/SAX/trozoG/heatmap/mapaCalor_bits=3.png)

![](img/SAX/trozoG/heatmap/mapaCalor_bits=4.png)

![](img/SAX/trozoG/heatmap/mapaCalor_bits=5.png)

![](img/SAX/trozoG/heatmap/mapaCalor_bits=6.png)

![](img/SAX/trozoG/heatmap/mapaCalor_bits=7.png)

![](img/SAX/trozoG/heatmap/mapaCalor_bits=8.png)

Mapa de color de la información mutua variando en función del número de 
bits, tamaño de palabra $w'$ y tamaño de alfabeto $a$.
:::


bits | words | alphabet_size | IM(LP,VD)
--- | --- | --- | ---    
5   |  66 | 50  | 15.460422  
6 | 66 | 5 | 5.318634  
6 | 66 | 2 | 5.180586  
7 | 66 | 5 | 5.112652  
7 | 66 | 2 | 5.077938  
5 | 93 | 5 | 5.059765  
8 | 66 | 5 | 4.921054  
8 | 66 | 2 | 4.911715  
4 | 66 | 5 | 4.905467    
4 | 93 | 5 | 4.890374  

: Combinaciones de información mutua máxima para el trozo G  {#tbl-resultados-sax-trozo-g-bits-top-im}

## Conclusiones trozo R y G 

Tal y como pueden observarse en los resultados obtenidos, la variación de los parámetros, plantea un mismo comportamiento que en el trozo C.
A mayor número de muestras involucradas en el experimento, peores resultados se obtienen de la información mutua. 

## Futuros campos que contemplar esta codificación   

A la hora de presentar el problema se indica que el tamaño final de la cadena es $w$, sería interesante reenfocarlo como nosotros hemos hecho, en vez de la media de los $w_0/w$, definir la variable $w'$ que sería el número de elementos sobre los que se haría la dicha media. 

La longitud, por tanto, vendría dada como $w_0/w'$ y lo que es más interesante 
se podría jugar con el *stride* (desplazamiento de ventana) y hacer de la media una pondera. 

Además el problema de utilizar percentiles equidistantes es que se pierde 
la posibilidad de que con un espacio menor de combinaciones posible entre todos los símbolos del alfabeto se pueda codificar un evento de cierto tipo, como por ejemplo los *spikes* en nuestro algoritmo. 


## Notas  

- Es muy similar a lo que nosotros hemos hecho, solo que este no discretiza en 0 y 1 solo (a no ser que se indique el alfabeto así)
y se usa un percentil homogéneo -> es mejor lo que hemos hecho nosotros para detectar los outliers. 

